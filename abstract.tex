\begin{abstract}
The primary objective in time series analysis is forecasting. Raw data often exhibits nonstationary behavior, trends, seasonal cycles, and heteroskedasticity. After data is transformed to a weakly stationary process, the autoregressive moving average (ARMA) model captures the remaining temporal dynamics to improve forecasting. Estimation of ARMA is simplified through regressing future values on known realizations and proxy innovations. The classic paradigm fails when nonlinearities remain. Regime-switching models classify changes in level, ARMA dynamics, and volatility using a finite number of latent states. If the states are describable using past endogenous or exogenous information, a threshold autoregressive (TAR) or logistic smooth transition autoregressive (LSTAR) model help simplify complex patterns to conditional weakly stationary processes. For ARMA, TAR, and STAR, order parameters quantify the extent past information impacts the future. Unfortunately, even if model orders are known \textit{a priori}, the possibility of overfitting can lead to sub-optimal forecasting performance. By intentionally overestimating these orders, we exploit the linear representation of the full models and use Bayesian regularization to achieve sparsity. Global-local shrinkage priors for AR, MA, and exogenous coefficients are adopted to pull posterior means toward 0 without over-shrinking relevant effects. This dissertation introduces, evaluates, and compares Bayesian techniques that automatically perform model selection and coefficient estimation of ARMA, TAR, and STAR models. Multiple Monte Carlo experiments illustrate the accuracy of these methods in finding the "true" data generating process. Practical applications demonstrate their efficacy in forecasting.

\end{abstract}
