\chapter{Introduction}
In order of historical discovery, the autoregressive moving average, threshold autoregressive, and logistic smooth transition autoregressive processes have been extensively studied for the modeling and forecasting of linear and nonlinear time series. All three models are parametric and easy to interpret making them popular in application. Order parameters control the overall complexity of these models and often require estimation. By intentionally overestimating these order parameters, Bayesian regularization and selection methods can be applied for flexible subset estimation of these parametric models. 

The logistic smooth transition autoregressive (LSTAR) model is a regime-switching nonlinear time series specification that has been adopted in a wide variety of applications. LSTAR is formulated as a weighted combination of two or more linear autoregressive (AR) processes. In Chapter \ref{chap:temp}, LSTAR models are estimated using Bayesian shrinkage ($Laplace$ and $Horseshoe$) priors on the autoregressive coefficients of each regime and \textit{Dirichlet} priors are employed to identify composite threshold variables in the transition function. The proposed specification provides a flexible alternative to time-consuming stepwise model building procedures and to computationally intensive reversible jump Markov chain Monte Carlo (RJMCMC) schemes. A series of experiments is presented to demonstrate the efficacy of the methodology, which can be applied in existing Bayesian software packages. Application to a classic nonlinear time series illustrates the ability to achieve superior forecasting performance. Finally, the capability to handle multiple input exogenous time series is exemplified through forecasting daily maximum water temperatures: for 31 Spanish rivers, Bayesian estimates of linear and nonlinear river-specific models are evaluated with regard to their 3-step and 7-step ahead forecasting performance.

Urban traffic patterns naturally change with the growing populations of metropolitan areas. Real-time management systems capture high frequency traffic data to obtain short-term forecasts of critical traffic variables.  For example, traffic occupancy measures vehicular density in an arterial through the percentage of time a sensor detects a vehicle. Major research over the last 20 years focused primarily on the modeling and forecasting of traffic volume. Like traffic volume,  occupancy is a useful metric for quantifying traffic concentration that exhibits weekly seasonal patterns, nonlinear dynamics, and heteroskedasticity. Multiple-regime threshold autoregressive models (TAR), reformulated as high dimensional linear regressions, help understand the changing temporal dynamics as traffic flows between different levels of congestion. In Chapter \ref{chap:traffic}, a Bayesian three step model building procedure is used for parsimonious estimation of subset TAR models designed for day-specific and horizon-specific (1-step, 3-step, and 5-step ahead) forecasting of traffic occupancy at 7 detector locations. In the first step, fully saturated multiple regime TAR models are fitted using Bayesian horseshoe priors for sparse estimation. Next, regimes are selected through a forward stepwise selection algorithm based on the Kullback-Leibler (KL) distance between the posterior predictive distribution of the full reference model and a TAR model with fewer regimes. Given the regimes, the forward selection algorithm is repeated to ensure the most parsimonious model is selected. Empirical results applied to traffic data from Athens, Greece, establish the efficacy of these procedures in obtaining interpretable models designed to produce point and density forecasts at multiple horizons.

The autoregressive moving average (ARMA) model is valuable in describing and forecasting weakly stationary stochastic processes. Classic ARMA model selection relies on choosing AR order $p$ and MA order $q$ to minimize prediction error (PE). Information criteria such as AIC or BIC discourage overfitting to estimate PE. The subset ARMA$(p,q)$ model is more flexible but often involves  computationally intensive methods for model selection. Treating ARMA as a linear regression model, regularization techniques are explored and evaluated to automatically select and estimate subset ARMA$(p,q)$ in Chapter \ref{chap:co2}. Because of temporal dependence, procedures considered are capabable of handling the natural multicollinearity existant in AR and MA predictors. Extended from the adaptive LASSO (ADLASSO) used in \cite{Chen2011}, the adaptive elastic net (ADENET) which combines $\ell_1$ and $\ell_2$ regularization is considered. Beyond AIC and BIC, cross-validation techniques estimate PE and aid in final model selection. Under the Bayesian framework, horseshoe (HS) priors are valuable in sparse estimation of a full ARMA$(p,q)$ reference model. Posterior distributions of sub models are quickly obtainable through projection, and discrepancy is measured by the Kullback-Leibler distance. A forward selection algorithm identifies the best nested sequence of subset ARMA$(p,q)$ models, and the final model is chosen based on estimated PE. For the full library of methods discussed, model selection is evaluated via simulation and forecasting performance via practical application.