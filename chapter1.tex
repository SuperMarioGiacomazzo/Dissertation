\chapter{Introduction}
The logistic smooth transition autoregressive (LSTAR) model is a regime-switching nonlinear time series specification that has been adopted in a wide variety of applications. LSTAR is formulated as a weighted combination of two or more linear autoregressive (AR) processes. In Chapter \ref{chap:temp}, LSTAR models are estimated using Bayesian shrinkage ($Laplace$ and $Horseshoe$) priors on the autoregressive coefficients of each regime and \textit{Dirichlet} priors are employed to identify composite threshold variables in the transition function. The proposed specification provides a flexible alternative to time-consuming stepwise model building procedures and to computationally intensive reversible jump Markov chain Monte Carlo (RJMCMC) schemes. A series of experiments is presented to demonstrate the efficacy of the methodology, which can be applied in existing Bayesian software packages. Application to a classic nonlinear time series illustrates the ability to achieve superior forecasting performance. Finally, the capability to handle multiple input exogenous time series is exemplified through forecasting daily maximum water temperatures: for 31 Spanish rivers, Bayesian estimates of linear and nonlinear river-specific models are evaluated with regard to their 3-step and 7-step ahead forecasting performance.

Urban traffic patterns naturally change with the growing populations of metropolitan areas. Real-time management systems capture high frequency traffic data to obtain short-term forecasts of critical traffic variables.  For example, traffic occupancy measures vehicular density in an arterial through the percentage of time a sensor detects a vehicle. Major research over the last 20 years focused primarily on the modeling and forecasting of traffic volume. Like traffic volume,  occupancy is a useful metric for quantifying traffic concentration that exhibits weekly seasonal patterns, space-time dependencies, nonlinear dynamics, and heteroskedasticity. In Chapter \ref{chap:traffic}, we utilize a Bayesian three step model building procedure for parsimonious estimation of day-specific models designed for horizon-specific (1-step, 3-step, and 5-step) forecasting of traffic occupancy. In the first step, we fit competing models using Bayesian horseshoe priors for sparse estimation. To capture temporal information downstream, upstream, and at the interested location, we primarily compare linear autoregressive distributed lag models (ARDLs) to nonlinear self-exciting threshold autoregressive distributed lag models (SETARDLs).  Next, the optimal sub-model at each level of flexibility from intercept-only to full is identified through a forward step-wise procedure measuring value by minimization of the Kullback-Leibler (KL) divergence between the full reference model to a proposed smaller model. Finally, the best model is chosen based forecasting accuracy. Evaluation of the best ARDL and SETARDL models is conducted using a time period held out of the model estimation and selection periods. Empirical results applied to traffic data from Athens, Greece establishes the efficacy of these procedures in obtaining interpretable models designed to forecast at multiple horizons.

The autoregressive moving average (ARMA) model is valuable in describing and forecasting weakly stationary stochastic processes. Classic ARMA model selection relies on choosing AR order $p$ and MA order $q$ to minimize prediction error (PE). Information criteria such as AIC or BIC discourage overfitting to estimate PE. The subset ARMA$(p,q)$ model is more flexible but often involves  computationally intensive methods for model selection. Treating ARMA as a linear regression model, we explore and evaluate regularization techniques that automate selection and estimation of subset ARMA$(p,q)$. Because of temporal dependence, procedures considered are capabable of handling the natural multicollinearity existant in AR and MA predictors. We extend from the adaptive LASSO (ADLASSO) used in \cite{Chen2011} to the adaptive elastic net (ADENET) which combines $\ell_1$ and $\ell_2$ regularization. Beyond AIC and BIC, we illustrate cross-validation techniques to estimate PE and aid in final model selection. Under the Bayesian framework, horseshoe (HS) priors are valuable in sparse estimation of a full ARMA$(p,q)$ reference model. Posterior distributions of sub models are quickly obtainable through projection, and discrepancy is measured by the Kullback-Leibler distance. A forward selection algorithm identifies the best nested sequence of subset ARMA$(p,q)$ models, and the final model is chosen based on estimated PE. For the full library of methods discussed, model selection is evaluated via simulation and forecasting performance via practical application.